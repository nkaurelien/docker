{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a844c5-2372-4044-b421-2e3a8f9bbb8d",
   "metadata": {},
   "source": [
    "INTRODUCTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb8cec-fa3f-46e6-a6af-af2d66b18819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pwd = os.getcwd()\n",
    "\n",
    "display(pwd)\n",
    "\n",
    "import pyspark\n",
    "\n",
    "# create spqrt context\n",
    "sc =  pyspark.SparkContext(appName=\"FichesPratiquesDT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc562b-3d3d-4d40-85f9-931e90e5647a",
   "metadata": {},
   "source": [
    "CREATION DU SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e3c2e-b70b-422c-9381-88450e03abfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e555b6fd-7c80-453d-b2a9-5980512d7516",
   "metadata": {},
   "source": [
    "CHARGER LE JSON A PARTIR D'UN LIEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da303b4-0f63-4ca2-b480-5cb165a38db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"FichesPratiquesDT\").getOrCreate()\n",
    "\n",
    "# Step 1: Fetch JSON data from the URL\n",
    "# response = requests.get(\"http://api.luftdaten.info/static/v1/data.json\" )\n",
    "\n",
    "\n",
    "# URL of the JSON file\n",
    "url = \"https://raw.githubusercontent.com/SocialGouv/fiches-travail-data/master/data/fiches-travail.json\"\n",
    "# Fetch JSON data from the URL\n",
    "response = requests.get(url)\n",
    "json_data = response.json()  # Parse the JSON data\n",
    "\n",
    "# display(json_data[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3e7c4b-a87e-413b-8470-47e83da145dc",
   "metadata": {},
   "source": [
    "CHARGER LE JSON A PARTIR DU REPERTOIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7fd00975-25ec-4202-a1a2-937c6710945c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      date|         description|               intro|        pubId|      section_anchor| section_description|        section_text|       section_title|               title|                 url|\n",
      "+----------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|02/08/2024|Chaque syndicat r...|<p>Chaque syndica...|article375542|                    |À savoir ! Le man...| À savoir ! Le ma...|Les délégués synd...|Les délégués synd...|https://travail-e...|\n",
      "|02/08/2024|Chaque syndicat r...|<p>Chaque syndica...|article375542|L-action-des-synd...|L'action des synd...|L'action des synd...|L’action des synd...|Les délégués synd...|https://travail-e...|\n",
      "|14/06/2024|Le contrat de tra...|<p><strong>Le con...|article100979|                    |A savoir ! Lorsqu...| A savoir ! Lorsq...|Le contrat à duré...|Le contrat à duré...|https://travail-e...|\n",
      "|14/06/2024|Le contrat de tra...|<p><strong>Le con...|article100979|Le-contrat-a-dure...|La conclusion du ...|La conclusion du ...|Le contrat à duré...|Le contrat à duré...|https://travail-e...|\n",
      "|20/06/2024|La conclusion d'u...|<p>La conclusion ...|article100982|Les-entreprises-d...|Les entreprises d...|Les entreprises d...|Les entreprises d...|Le contrat de tra...|https://travail-e...|\n",
      "|06/12/2023|Le travail saison...|<p>Le travail sai...|article100986|                    |A SAVOIR Sauf con...| A SAVOIR Sauf co...|Le travail saison...|Le travail saison...|https://travail-e...|\n",
      "|06/12/2023|Le travail saison...|<p>Le travail sai...|article100986|Le-terme-du-contr...|Certains CDD, par...|Certains CDD, par...|Le terme du contr...|Le travail saison...|https://travail-e...|\n",
      "|06/12/2023|Le travail saison...|<p>Le travail sai...|article100986|La-succession-de-...|Renouveler un con...|Renouveler un con...|La succession de ...|Le travail saison...|https://travail-e...|\n",
      "|06/12/2023|Le travail saison...|<p>Le travail sai...|article100986|La-clause-de-reco...|Le contrat de tra...|Le contrat de tra...|La clause de reco...|Le travail saison...|https://travail-e...|\n",
      "|18/07/2022|Type particulier ...|<p>Type particuli...|article100987| La-duree-du-contrat|La durée du contr...|La durée du contr...| La durée du contrat|Le contrat vendanges|https://travail-e...|\n",
      "|18/07/2022|Type particulier ...|<p>Type particuli...|article100987|La-possibilite-po...|Un même salarié p...|Un même salarié p...|La possibilité po...|Le contrat vendanges|https://travail-e...|\n",
      "|03/06/2024|Deux situations d...|<p>Deux situation...|article100988|                    |À savoir Même si ...| À savoir Même si...|La modification d...|La modification d...|https://travail-e...|\n",
      "|21/07/2022|Le changement de ...|<p>Le changement ...|article100989|                    |A SAVOIR Les sala...| A SAVOIR Les sal...|Le changement de ...|Le changement de ...|https://travail-e...|\n",
      "|16/07/2024|La réglementation...|<p>La réglementat...|article101013|                    |ACTUALITÉS L’arrê...| ACTUALITÉS L’arr...|             Amiante|             Amiante|https://travail-e...|\n",
      "|16/07/2024|La réglementation...|<p>La réglementat...|article101013|Plan-d-actions-in...|Le plan d’actions...|Le plan d’actions...|Plan d’actions in...|             Amiante|https://travail-e...|\n",
      "|16/07/2024|La réglementation...|<p>La réglementat...|article101013|Plan-pluriannuel-...|L’ampleur des cha...|L’ampleur des cha...|Plan pluriannuel ...|             Amiante|https://travail-e...|\n",
      "|21/06/2024|Les salariés repr...|<p>Les salariés r...|article101094|                    |À savoir ! L’auto...| À savoir ! L’aut...|La protection en ...|La protection en ...|https://travail-e...|\n",
      "|31/05/2024|La démission perm...|<p><strong>La dém...|article101095|                    |À savoir ! Le cod...| À savoir ! Le co...|        La démission|        La démission|https://travail-e...|\n",
      "|23/05/2024|Constitue un lice...|<p>Constitue un l...|article101099|La-definition-du-...|La définition du ...|La définition du ...|La définition du ...|La définition du ...|https://travail-e...|\n",
      "|23/05/2024|Constitue un lice...|<p>Constitue un l...|article101099|Les-difficultes-e...|Suppression ou tr...|Suppression ou tr...|Les difficultés é...|La définition du ...|https://travail-e...|\n",
      "+----------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"intro\", StringType(), True),\n",
    "    StructField(\"pubId\", StringType(), True),\n",
    "    StructField(\"section_anchor\", StringType(), True),\n",
    "    StructField(\"section_html\", StringType(), True),\n",
    "    StructField(\"section_text\", StringType(), True),\n",
    "    StructField(\"section_title\", StringType(), True),\n",
    "    StructField(\"section_description\", StringType(), True),\n",
    "    StructField(\"section_references\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "def flatten_filter_and_map_json_data(records):\n",
    "    filtered_records = []\n",
    "    for record in records:\n",
    "        # Copy all fields except 'sections'\n",
    "        new_record = {key: value for key, value in record.items() if key != 'sections'}\n",
    "        \n",
    "        # Filter 'sections' if it exists and flatten it\n",
    "        # Ensure sections exist and is a list\n",
    "        if 'sections' in record and isinstance(record['sections'], list):\n",
    "            for section in record['sections']:\n",
    "                if isinstance(section, dict):  # Ensure each section is a dictionary\n",
    "                    flat_section = {f\"section_{key}\": value for key, value in section.items() if key in ['title', 'text', 'anchor', 'description']}\n",
    "                    combined_record = {**new_record, **flat_section}\n",
    "                    filtered_records.append(combined_record)\n",
    "        else:\n",
    "            filtered_records.append(new_record)\n",
    "    return filtered_records\n",
    "\n",
    "# Apply the filtering and mapping function\n",
    "data = flatten_filter_and_map_json_data(json_data)\n",
    "\n",
    "# Convert the list of dictionaries to JSONLines format\n",
    "# data = \"\\n\".join([json.dumps(record) for record in data])\n",
    "\n",
    "# Step 2: Convert JSON data to an RDD\n",
    "rdd = spark.sparkContext.parallelize(data[:1000])\n",
    "data = None\n",
    "\n",
    "# Print the content of the RDD\n",
    "# for record in rdd.collect():\n",
    "#     print(record)\n",
    "\n",
    "\n",
    "# Step 3: Convert RDD to DataFrame\n",
    "# df = spark.read.json(rdd,  multiLine=True)\n",
    "# df = spark.read.schema(schema).json(rdd,  multiLine=True)\n",
    "\n",
    "df = spark.read.json(rdd, multiLine=False)\n",
    "\n",
    "# Filter out rows with `_corrupt_record` and select only valid rows\n",
    "df = df.filter(col(\"_corrupt_record\").isNull()).drop(\"_corrupt_record\")\n",
    "\n",
    "# Print the schema of the DataFrame\n",
    "# df.printSchema()\n",
    "# \n",
    "# Show the first 2 rows of the DataFrame\n",
    "df.show(20, truncate=True)\n",
    "\n",
    "# print df head\n",
    "# display(df.head(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e94cade4-a878-4da7-87d6-42a8374ebb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- intro: string (nullable = true)\n",
      " |-- pubId: string (nullable = true)\n",
      " |-- section_anchor: string (nullable = true)\n",
      " |-- section_description: string (nullable = true)\n",
      " |-- section_text: string (nullable = true)\n",
      " |-- section_title: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n",
      "\n",
      "Basic statistics (describe):\n",
      "+-------+----------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|summary|      date|         description|               intro|        pubId|      section_anchor| section_description|        section_text|       section_title|               title|                 url|\n",
      "+-------+----------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  count|       371|                 371|                 371|          371|                 371|                 371|                 371|                 371|                 371|                 371|\n",
      "|   mean|      NULL|                NULL|                NULL|         NULL|                NULL|                NULL|                NULL|                NULL|                NULL|                NULL|\n",
      "| stddev|      NULL|                NULL|                NULL|         NULL|                NULL|                NULL|                NULL|                NULL|                NULL|                NULL|\n",
      "|    min|01/07/2024|Activités exercée...|                    |article100979|                    |                    |                    |Accès aux avis du...|Acide chlorhydriq...|https://travail-e...|\n",
      "|    max|31/05/2024|Être salarié d'un...|<p>Être <strong>s...|article375542|sources-reglement...|Âge d’entrée en a...|Âge d’entrée en a...|Écoles de la deux...|Écoles de la deux...|https://travail-e...|\n",
      "+-------+----------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "Detailed summary statistics:\n",
      "+-------+----------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|summary|      date|         description|               intro|        pubId|      section_anchor| section_description|        section_text|       section_title|               title|                 url|\n",
      "+-------+----------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  count|       371|                 371|                 371|          371|                 371|                 371|                 371|                 371|                 371|                 371|\n",
      "|   mean|      NULL|                NULL|                NULL|         NULL|                NULL|                NULL|                NULL|                NULL|                NULL|                NULL|\n",
      "| stddev|      NULL|                NULL|                NULL|         NULL|                NULL|                NULL|                NULL|                NULL|                NULL|                NULL|\n",
      "|    min|01/07/2024|Activités exercée...|                    |article100979|                    |                    |                    |Accès aux avis du...|Acide chlorhydriq...|https://travail-e...|\n",
      "|    25%|      NULL|                NULL|                NULL|         NULL|                NULL|                NULL|                NULL|                NULL|                NULL|                NULL|\n",
      "|    50%|      NULL|                NULL|                NULL|         NULL|                NULL|                NULL|                NULL|                NULL|                NULL|                NULL|\n",
      "|    75%|      NULL|                NULL|                NULL|         NULL|                NULL|                NULL|                NULL|                NULL|                NULL|                NULL|\n",
      "|    max|31/05/2024|Être salarié d'un...|<p>Être <strong>s...|article375542|sources-reglement...|Âge d’entrée en a...|Âge d’entrée en a...|Écoles de la deux...|Écoles de la deux...|https://travail-e...|\n",
      "+-------+----------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "Total number of rows: 371\n",
      "\n",
      "Null values count per column:\n",
      "+----+-----------+-----+-----+--------------+-------------------+------------+-------------+-----+---+\n",
      "|date|description|intro|pubId|section_anchor|section_description|section_text|section_title|title|url|\n",
      "+----+-----------+-----+-----+--------------+-------------------+------------+-------------+-----+---+\n",
      "|   0|          0|    0|    0|             0|                  0|           0|            0|    0|  0|\n",
      "+----+-----------+-----+-----+--------------+-------------------+------------+-------------+-----+---+\n",
      "\n",
      "\n",
      "Column names and data types:\n",
      "date: string\n",
      "description: string\n",
      "intro: string\n",
      "pubId: string\n",
      "section_anchor: string\n",
      "section_description: string\n",
      "section_text: string\n",
      "section_title: string\n",
      "title: string\n",
      "url: string\n",
      "\n",
      "Distinct values count per column:\n",
      "+----+-----------+-----+-----+--------------+-------------------+------------+-------------+-----+---+\n",
      "|date|description|intro|pubId|section_anchor|section_description|section_text|section_title|title|url|\n",
      "+----+-----------+-----+-----+--------------+-------------------+------------+-------------+-----+---+\n",
      "|  70|        153|  107|  154|           123|                366|         368|          207|  153|153|\n",
      "+----+-----------+-----+-----+--------------+-------------------+------------+-------------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, when, countDistinct\n",
    "\n",
    "# Print the schema of the DataFrame\n",
    "print(\"Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Show basic statistics for numerical columns\n",
    "print(\"\\nBasic statistics (describe):\")\n",
    "df.describe().show()\n",
    "\n",
    "# Show detailed summary statistics\n",
    "print(\"\\nDetailed summary statistics:\")\n",
    "df.summary().show()\n",
    "\n",
    "# Count total number of rows\n",
    "row_count = df.count()\n",
    "print(f\"\\nTotal number of rows: {row_count}\")\n",
    "\n",
    "\n",
    "# Check for null values\n",
    "print(\"\\nNull values count per column:\")\n",
    "df.select([sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Show column names and data types\n",
    "print(\"\\nColumn names and data types:\")\n",
    "for col_name, dtype in df.dtypes:\n",
    "    print(f\"{col_name}: {dtype}\")\n",
    "\n",
    "# Count distinct values in each column\n",
    "print(\"\\nDistinct values count per column:\")\n",
    "df.select([countDistinct(c).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c7603c-a6d2-4086-be2f-2b0d7b78050a",
   "metadata": {},
   "source": [
    "# Write the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e4d2b003-ac88-4198-8125-13f83b27219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"output.csv\")\n",
    "\n",
    "# Save the DataFrame to a JSON file without truncation\n",
    "df.write.mode(\"overwrite\").json(\"output.json\")\n",
    "\n",
    "# If you need a single JSON file:\n",
    "df.coalesce(1).write.mode(\"overwrite\").json(\"single_output.json\")\n",
    "\n",
    "# Save the DataFrame as a JSONL file\n",
    "df.write.mode(\"overwrite\").json(\"output.jsonl\", lineSep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87395a2-99c4-424d-afa5-38222ab3636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"intro\", StringType(), True),\n",
    "    StructField(\"pubId\", StringType(), True),\n",
    "    StructField(\"sections\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"anchor\", StringType(), True),\n",
    "            StructField(\"html\", StringType(), True),\n",
    "            StructField(\"text\", StringType(), True),\n",
    "            StructField(\"title\", StringType(), True),\n",
    "            StructField(\"description\", StringType(), True),\n",
    "            StructField(\"references\", MapType(StringType(), StringType()), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "\n",
    "# mapping and filtering\n",
    "def filter_and_map_json_data(records):\n",
    "    filtered_records = []\n",
    "    for record in records:\n",
    "        # Copy all fields except 'sections'\n",
    "        new_record = {key: value for key, value in record.items() if key != 'sections'}\n",
    "        \n",
    "        # # Filter 'sections' if it exists\n",
    "        # if 'sections' in record:\n",
    "        #     new_record['sections'] = [\n",
    "        #         {key: value for key, value in section.items() if key in ['title', 'text', 'anchor', 'description']}\n",
    "        #         for section in record['sections']\n",
    "        #     ]\n",
    "        \n",
    "        filtered_records.append(new_record)\n",
    "    return filtered_records\n",
    "\n",
    "# Apply the filtering and mapping function\n",
    "filtered_data = filter_and_map_json_data(json_data)\n",
    "\n",
    "# Step 2: Convert JSON data to an RDD\n",
    "rdd = spark.sparkContext.parallelize([filtered_data[:1]])\n",
    "\n",
    "# Step 3: Convert RDD to DataFrame\n",
    "df = spark.read.json(rdd,  multiLine=True)\n",
    "# df = spark.read.schema(schema).json(rdd,  multiLine=True)\n",
    "\n",
    "# Print the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# print df head\n",
    "display(df.head(2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
